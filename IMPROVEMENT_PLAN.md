IMPROVEMENT_PLAN.md
RAE Benchmarking â€“ Upgrade Plan (Target Quality: 6/5)
Complete roadmap for transforming RAE benchmarks into an academic- and enterprise-grade evaluation suite
ğŸ¯ Cel dokumentu

Celem tego planu jest podniesienie moduÅ‚u Benchmarking & Evaluation w projekcie RAE do poziomu:

standardu akademickiego (AGH, PK, UJ, conference-level reproducibility),

standardu korporacyjnego (Canon R&D, Minolta Labs, Motorola, AbakusAI),

standardu open-source premium (czytelna struktura, Å‚atwy start, gotowe zestawy).

Po wdroÅ¼eniu wszystkich punktÃ³w, projekt osiÄ…ga poziom 6/5:

Benchmarking nie jest dodatkiem â€” benchmarking staje siÄ™ peÅ‚noprawnym produktem w produkcie.

ğŸ§± 1. Struktura katalogÃ³w â€“ docelowy ukÅ‚ad
benchmarking/
â”‚
â”œâ”€â”€ BENCHMARK_STARTER.md
â”œâ”€â”€ BENCHMARK_REPORT_TEMPLATE.md
â”œâ”€â”€ IMPROVEMENT_PLAN.md  â† ten plik
â”‚
â”œâ”€â”€ sets/
â”‚   â”œâ”€â”€ academic_lite.yaml
â”‚   â”œâ”€â”€ academic_extended.yaml
â”‚   â”œâ”€â”€ industrial_small.yaml
â”‚   â””â”€â”€ industrial_large_template.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_benchmark.py
â”‚   â”œâ”€â”€ compare_runs.py
â”‚   â””â”€â”€ plot_metrics.py   (opcjonalne)
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ example_report.md
    â””â”€â”€ example_metrics.json

ğŸ§ª 2. Lista funkcjonalna â€“ co benchmarki muszÄ… mierzyÄ‡
2.1 Metryki jakoÅ›ci
Metryka	Opis
HitRate@k	Czy wÅ‚aÅ›ciwa pamiÄ™Ä‡ znajduje siÄ™ w top-k
MRR	Åšrednia odwrotnej pozycji trafienia
Precision@k	DokÅ‚adnoÅ›Ä‡ odpowiedzi
Recall@k	Pokrycie ÅºrÃ³deÅ‚
Semantic Similarity Score	JakoÅ›Ä‡ embedderÃ³w
2.2 Metryki wydajnoÅ›ci

Latencja Å›rednia

Latencja P95 / P99

Throughput (zapytania/sekundÄ™)

Czas dodania wpisÃ³w do pamiÄ™ci

Koszt tokenÃ³w (jeÅ›li uÅ¼ywany LLM)

2.3 Metryki wewnÄ™trzne RAE

WpÅ‚yw refleksji na jakoÅ›Ä‡ pamiÄ™ci

WpÅ‚yw pruning/summarization

WpÅ‚yw GraphRAG

WpÅ‚yw rÃ³Å¼nych konfiguracji top_k

ğŸ”§ 3. Minimalne wymagania do poziomu 6/5

To jest najwaÅ¼niejsza sekcja â€“ implementujesz jÄ… 1:1.

âœ” 3.1. Dodanie 3 oficjalnych benchmark sets

Dodaj w katalogu benchmarking/sets/:

academic_lite.yaml

â†’ szybki test w <10 sekund
â†’ 3 memories, 5 queries
â†’ dla maÅ‚ych maszyn

academic_extended.yaml

â†’ 25â€“50 memories
â†’ 20 queries
â†’ odpornoÅ›Ä‡ na szum i podobne pojÄ™cia

industrial_small.yaml

â†’ 100â€“300 memories
â†’ prawdziwe â€brudne daneâ€
â†’ test GraphRAG i refleksji

âœ” 3.2. Oficjalny skrypt do uruchamiania benchmarkÃ³w

benchmarking/scripts/run_benchmark.py:

Å‚aduje YAML

wykonuje memories â†’ insert

queries â†’ search

zbiera metryki

zapisuje metrics.json i report.md

Polecenie:

python benchmarking/scripts/run_benchmark.py --set academic_lite.yaml

âœ” 3.3. Compare engine

compare_runs.py â€“ porÃ³wnuje dwa wyniki:

python compare_runs.py runA.json runB.json


Wyniki:

rÃ³Å¼nice w MRR

rÃ³Å¼nice w jakoÅ›ci

rÃ³Å¼nice w latencji

wykresy (opcjonalnie)

âœ” 3.4. Makefile targets

Dodaj:

benchmark-lite:
	.venv/bin/python benchmarking/scripts/run_benchmark.py --set academic_lite.yaml

benchmark-full:
	.venv/bin/python benchmarking/scripts/run_benchmark.py --set academic_extended.yaml

benchmark-industrial:
	.venv/bin/python benchmarking/scripts/run_benchmark.py --set industrial_small.yaml

âœ” 3.5. Integracja z CI/CD

W pliku workflow:

dodaj job benchmark-smoke odpalany przy pull requestach

limit czasu: 60 sekund

tylko academic_lite.yaml

Efekt:
PR nie przejdzie, jeÅ›li benchmark siÄ™ pogorszyÅ‚.

âœ” 3.6. Dashboard integracja

W dashboardzie (opcjonalne po wdroÅ¼eniu):

sekcja â€Benchmark Resultsâ€

tabelka z ostatnimi wynikami

wykres trendu MRR i latencji

ğŸ“ˆ 4. Jak wyglÄ…da benchmark klasy 6/5

PrzykÅ‚ad raportu, ktÃ³ry wyglÄ…da jak z laboratorium AGH / Google Research:

RAE Benchmark Report (ACADEMIC EXTENDED)
Machine: Intel i7, 16GB RAM
Config: RAE Lite, Reflection Engine ON

Dataset: 50 memories, 20 queries
Run time: 1.94 sec

Quality:
- HitRate@5: 0.84
- MRR: 0.71
- Semantic Precision: 0.88

Performance:
- Avg Latency: 44ms
- P95 Latency: 79ms
- Insert Time: 0.12s

Observations:
- Reflection improves MRR by +0.06
- GraphRAG improves entity alignment


To jest standard, ktÃ³ry kaÅ¼dy naukowiec rozumie, a firma widzi od razu:

â€To jest produkt przemyÅ›lany.â€

ğŸ“œ 5. Checklista do zrobienia (w kolejnoÅ›ci)
Faza 1 â€“ struktura

 UtworzyÄ‡ katalog benchmarking/sets/

 DodaÄ‡ 3 zestawy YAML

 DodaÄ‡ skrypt run_benchmark.py

 DodaÄ‡ compare_runs.py

 DodaÄ‡ BENCHMARK_STARTER.md i BENCHMARK_REPORT_TEMPLATE.md

Faza 2 â€“ automatyzacja

 DodaÄ‡ targety w Makefile

 DodaÄ‡ job benchmark-smoke do GitHub Actions

 DodaÄ‡ minimalny wynik w badge w README

Faza 3 â€“ â€wow factorâ€

 DodaÄ‡ do dashboardu sekcjÄ™ â€Benchmark Resultsâ€

 DodaÄ‡ kolorowe wykresy (lite)

 PrzygotowaÄ‡ example_report.md z wynikiem referencyjnym

Po wdroÅ¼eniu wszystkich punktÃ³w:

ğŸ‰ Benchmarking Suite w RAE osiÄ…ga poziom 6/5
To poziom projektÃ³w klasy Google Research, Meta FAIR, Anthropic evals.

ğŸ 6. Gotowy komunikat do repo

W README moÅ¼esz dopisaÄ‡ ten blok:

### ğŸ”¬ Academic & Enterprise Benchmarking Suite
RAE includes a fully structured benchmarking environment:

- 3 official benchmark datasets (lite, academic, industrial)
- Automated scripts to run and compare results
- GitHub Actions benchmarking smoke tests
- Research-grade evaluation metrics (MRR, HitRate@k, latency, semantic precision)
- Report templates for university labs

See: /benchmarking/IMPROVEMENT_BENCHMARK_PLAN.md